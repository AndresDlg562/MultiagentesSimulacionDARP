{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sembradora 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It demonstrates how to use the [agentpy](https://agentpy.readthedocs.io) package to create and visualize networks, use the interactive module, and perform different types of sensitivity analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model design\n",
    "import agentpy as ap\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from queue import PriorityQueue\n",
    "from itertools import count\n",
    "import math\n",
    "\n",
    "# Visualization\n",
    "import imageio\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "# Guardar\n",
    "import pickle\n",
    "\n",
    "# Sistema\n",
    "import os\n",
    "\n",
    "# Machine learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "0 is empty\n",
    "1 is tractor\n",
    "2 is obstacle\n",
    "3 is target\n",
    "4 is seeds\n",
    "5 is ready to plant\n",
    "\"\"\"\n",
    "\n",
    "def is_connected(grid, free_positions):\n",
    "    \"\"\"Check if all free cells are connected using BFS.\"\"\"\n",
    "    n = grid.shape[0]\n",
    "    visited = set()\n",
    "    queue = deque([free_positions.pop()])\n",
    "    visited.add(queue[0])\n",
    "\n",
    "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    \n",
    "    connected_count = 0\n",
    "    free_count = len(free_positions)\n",
    "\n",
    "    while queue:\n",
    "        x, y = queue.popleft()\n",
    "        for dx, dy in directions:\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            if (0 <= new_x < n and 0 <= new_y < n and \n",
    "                (new_x, new_y) in free_positions and (new_x, new_y) not in visited):\n",
    "                queue.append((new_x, new_y))\n",
    "                visited.add((new_x, new_y))\n",
    "                connected_count += 1\n",
    "                \n",
    "    return connected_count == free_count\n",
    "\n",
    "def is_adjacent(pos1, pos2):\n",
    "    return abs(pos1[0] - pos2[0]) <= 1 and abs(pos1[1] - pos2[1]) <= 1\n",
    "\n",
    "def generate_grid(model, n, obstacles_count, crops_count):\n",
    "    \"\"\"Generate a grid with obstacles, ensuring free cells are connected.\"\"\"\n",
    "    while True:\n",
    "        grid = ap.Grid(model, (n, n), track_empty=True)\n",
    "        grid.add_field(\"occupied\", 0)\n",
    "\n",
    "        obstacle_positions = set()\n",
    "        while len(obstacle_positions) < obstacles_count:\n",
    "            pos = (random.randint(0, n-2), random.randint(0, n-2))\n",
    "            if pos not in obstacle_positions:\n",
    "                # Check if pos is adjacent to any seed position\n",
    "                is_valid = True\n",
    "                for seed_pos in model.p.seedsPositions:\n",
    "                    if is_adjacent(pos, seed_pos):\n",
    "                        is_valid = False\n",
    "                        break\n",
    "                if is_valid:\n",
    "                    obstacle_positions.add(pos)\n",
    "\n",
    "        for pos in obstacle_positions:\n",
    "            grid[\"occupied\"][pos] = 1\n",
    "\n",
    "        free_positions = {(x, y) for x in range(n) for y in range(n) if grid[\"occupied\"][x, y] == 0}\n",
    "        \n",
    "        if is_connected(grid, free_positions):\n",
    "            break\n",
    "\n",
    "    for pos in obstacle_positions:\n",
    "        grid[\"occupied\"][pos] = 1\n",
    "        model.np_grid[pos] = 2\n",
    "        \n",
    "        crops_positions = set()\n",
    "        while len(crops_positions) < crops_count:\n",
    "            pos = (random.randint(0, n-1), random.randint(0, n-1))\n",
    "            if pos not in obstacle_positions and pos not in crops_positions:\n",
    "                crops_positions.add(pos)\n",
    "                model.np_grid[pos] = 3\n",
    "\n",
    "    agentlist = ap.AgentList(model, len(obstacle_positions), agent_type=2)\n",
    "    grid.add_agents(agentlist, obstacle_positions)\n",
    "    \n",
    "    model.grid = grid\n",
    "    \n",
    "    return obstacle_positions, crops_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente poner tierra bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class earthTractor(ap.Agent):\n",
    "    def setup(self):\n",
    "        \"\"\"Initialize agent parameters.\"\"\"\n",
    "        self.initialized = False\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset agent attributes.\"\"\"\n",
    "        if not self.initialized:\n",
    "            self._initialize_agent()\n",
    "        else:\n",
    "            self._restore_agent()\n",
    "\n",
    "    def _initialize_agent(self):\n",
    "        \"\"\"Initialize the agent's initial state.\"\"\"\n",
    "        self.earthed = 0\n",
    "        self.destroyed = False\n",
    "        self.type = 2\n",
    "        self.past_pos = (0, 0)\n",
    "        self.pos = (0, 0)\n",
    "        self.new_pos = (0, 0)\n",
    "        self.cell_state = 0 \n",
    "        self.last_action = 5 \n",
    "        self.last_earth_pos = None\n",
    "        self.steps_without_earth = 0\n",
    "        self.last_movAction = None\n",
    "        self.agentid = None\n",
    "        \n",
    "        # Initialize Q-learning parameters\n",
    "        self.q_table = np.zeros((self.p.grid_size, self.p.grid_size, 2, 4, 6, 6))\n",
    "        self.learning_rate = self.p.learning_rate\n",
    "        self.discount_factor = self.p.discount_factor\n",
    "        self.epsilon = self.p.epsilon\n",
    "        \n",
    "        self.planted = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        # API\n",
    "        self.visitedPositions = []\n",
    "        \n",
    "    def setCellState(self, pos):\n",
    "        if self.model.np_grid[pos] == 5:\n",
    "            self.cell_state = 1\n",
    "        else:\n",
    "            self.cell_state = 0\n",
    "    \n",
    "    def exitGrid(self):\n",
    "        self.pos = (None, None)\n",
    "        \n",
    "    def setNewPositionVerify(self, pos):\n",
    "        self.new_pos = pos\n",
    "        \n",
    "    def setNewPositionArray(self, pos):\n",
    "        self.visitedPositions.append(pos)\n",
    "        \n",
    "    def move(self, agentes, dx=0, dy=0):\n",
    "        \"\"\"Move the tractor by (dx, dy).\"\"\"\n",
    "        x, y = self.pos\n",
    "        new_pos = (x + dx, y + dy)\n",
    "\n",
    "        # Asegúrate de que la nueva posición está dentro de los límites de la cuadrícula\n",
    "        if 0 <= new_pos[0] < self.p.grid_size and 0 <= new_pos[1] < self.p.grid_size:\n",
    "            # Verifica si la nueva posición no es un obstáculo\n",
    "            if self.model.np_grid[new_pos] != 2 and new_pos != self.p.seedsPositions[0]:\n",
    "                # Verifica si algún otro agente ya tiene la nueva posición como su siguiente posición\n",
    "                self.setNewPositionVerify(new_pos)\n",
    "                for agente in agentes:\n",
    "                    if agente is not self:\n",
    "                        if agente.new_pos == new_pos or agente.pos == new_pos:\n",
    "                            self.setNewPositionArray(self.pos)\n",
    "                            return self.pos, False  # No se puede mover porque la posición está ocupada por otro agente\n",
    "                # Si no hay obstáculos ni agentes en la nueva posición, se mueve\n",
    "                self.past_pos = self.pos\n",
    "                self.pos = new_pos\n",
    "                self.setCellState(self.pos)\n",
    "                self.setNewPositionArray(self.pos)\n",
    "                return self.pos, True\n",
    "        self.setNewPositionArray(self.pos)\n",
    "        return self.pos, False  # No se puede mover porque está fuera de los límites o hay un obstáculo\n",
    "\n",
    "    def move_up(self, agents):\n",
    "        self.last_movAction = 0\n",
    "        return self.move(agents, dy=1)\n",
    "\n",
    "    def move_down(self, agents):\n",
    "        self.last_movAction = 1\n",
    "        return self.move(agents, dy=-1)\n",
    "\n",
    "    def move_left(self, agents):\n",
    "        self.last_movAction = 2\n",
    "        return self.move(agents, dx=-1)\n",
    "\n",
    "    def move_right(self, agents):\n",
    "        self.last_movAction = 3\n",
    "        return self.move(agents, dx=1)\n",
    "    \n",
    "    def exit(self, agents):\n",
    "        x, y = self.pos\n",
    "\n",
    "        # Verifica si el agente ya está en el borde del grid\n",
    "        if (x == 0 or x == self.p.grid_size - 1 or y == 0 or y == self.p.grid_size - 1) and 3 not in self.model.np_grid:\n",
    "            self.destroyed = True\n",
    "            return self.pos, True  \n",
    "        else:\n",
    "            return self.pos, False\n",
    "\n",
    "    def setEarth(self, agents):\n",
    "        \"\"\"Drop seeds at the current target position.\"\"\"\n",
    "        # Remove target from grid\n",
    "        if self.model.np_grid[self.pos] == 3:\n",
    "            self.planted += 1\n",
    "            self.model.np_grid[self.pos] = 5\n",
    "            self.earthed += 1\n",
    "            self.last_earth_pos = self.pos\n",
    "            self.steps_without_earth = 0\n",
    "            return self.pos, True\n",
    "        else:\n",
    "            return self.pos, False\n",
    "\n",
    "    def get_reward(self, action, accionado):\n",
    "        \"\"\"Calculate the reward based on the action and outcome.\"\"\"\n",
    "        reward = 0  # Base penalty for movement to encourage efficiency\n",
    "\n",
    "        if action in [0, 1, 2, 3]:  # Movement actions\n",
    "            if accionado:\n",
    "                reward -= 0 # Reduced penalty for valid movement\n",
    "            else:\n",
    "                reward -= 1000  # Strong penalty for hitting an obstacle or invalid move\n",
    "\n",
    "        elif action == 4:  # Earth action\n",
    "            if accionado:\n",
    "                reward += 1  # High reward for planting seeds\n",
    "            else:\n",
    "                reward -= 1000  # General penalty for unsuccessful action\n",
    "        \n",
    "        elif action == 5:  # Exit action\n",
    "            if accionado:\n",
    "                reward += 1000\n",
    "            else:\n",
    "                reward -= 1000\n",
    "                \n",
    "        # Penalize for not moving in a linear way\n",
    "        if action in [0, 1, 2, 3] and self.last_movAction is not None and self.last_movAction != action:\n",
    "            reward -= 5\n",
    "        \n",
    "        # Penalization for planting far from the last planting position\n",
    "        if self.last_earth_pos is not None:\n",
    "            last_x, last_y = self.last_earth_pos\n",
    "            current_x, current_y = self.pos\n",
    "            if abs(last_x - current_x) > 1 or abs(last_y - current_y) > 1:\n",
    "                reward -= 5\n",
    "                \n",
    "        \n",
    "        self.reward += reward\n",
    "        return reward\n",
    "\n",
    "        \n",
    "    def q_learning_update(self, state, action, reward, next_state):\n",
    "        pos_x, pos_y, closeAgent, leftResources, lastAction = state\n",
    "        next_pos_x, next_pos_y, next_closeAgent, next_leftResources, next_lastAction = next_state\n",
    "        \n",
    "        # Q-value actual considerando las celdas adyacentes\n",
    "        current_q = self.q_table[pos_x, pos_y, closeAgent, leftResources, lastAction, action]\n",
    "        # Max Q-value del siguiente estado\n",
    "        max_next_q = np.max(self.q_table[next_pos_x, next_pos_y, next_closeAgent, next_leftResources, next_lastAction])\n",
    "\n",
    "        # Actualizar el valor Q\n",
    "        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)\n",
    "        self.q_table[pos_x, pos_y, closeAgent, leftResources, lastAction, action] = new_q\n",
    "        \n",
    "        if self.destroyed:\n",
    "            self.exitGrid()\n",
    "\n",
    "    def acciones(self, action, agents):\n",
    "        \"\"\"Map action to the corresponding method.\"\"\"\n",
    "        action_map = {\n",
    "            0: self.move_up,\n",
    "            1: self.move_down,\n",
    "            2: self.move_left,\n",
    "            3: self.move_right,\n",
    "            4: self.setEarth,\n",
    "            5: self.exit\n",
    "        }\n",
    "        if action != 4:\n",
    "            self.steps_without_earth += 1\n",
    "        accion = action_map[action]\n",
    "        return accion(agents)\n",
    "\n",
    "    def closeAgents(self, agents):\n",
    "        for agente in agents:\n",
    "            if agente is not self:\n",
    "                if abs(agente.pos[0] - self.pos[0]) <= 2 and abs(agente.pos[1] - self.pos[1]) <= 2:\n",
    "                    return 1\n",
    "        return 0\n",
    "    \n",
    "    def leftResources(self):\n",
    "        remaining_resources = np.sum(self.model.np_grid == 0)\n",
    "        \n",
    "        if remaining_resources == 0:\n",
    "            return 0\n",
    "        elif remaining_resources < 0.3 * self.p.crops_count:\n",
    "            return 1\n",
    "        elif remaining_resources < 0.6 * self.p.crops_count:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    \n",
    "    def step(self, agents):\n",
    "        \"\"\"Execute a step in the agent's behavior.\"\"\"\n",
    "        if self.destroyed:\n",
    "            return \n",
    "            \n",
    "        # Definir el estado actual\n",
    "        closeAgents = self.closeAgents(agents)\n",
    "        leftResources = self.leftResources()\n",
    "        state = (self.pos[0], self.pos[1], closeAgents, leftResources, self.last_action)\n",
    "        \n",
    "        # Acción seleccionada usando epsilon-greedy\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice([0, 1, 2, 3, 4, 5])\n",
    "        else:\n",
    "            action = np.argmax(self.q_table[state[0], state[1], state[2], state[3], state[4]])\n",
    "            \n",
    "        # Ejecutar la acción y actualizar la Q-table\n",
    "        next_state, accionado = self.acciones(action, agents)\n",
    "        next_closeAgents = self.closeAgents(agents)\n",
    "        next_leftResources = self.leftResources()\n",
    "        next_last_action = self.last_action\n",
    "        \n",
    "        # Asegúrate de que next_state esté dentro de los límites\n",
    "        next_state = (next_state[0], next_state[1], next_closeAgents, next_leftResources, next_last_action)\n",
    "        reward = self.get_reward(action, accionado)\n",
    "        self.q_learning_update(state, action, reward, next_state)\n",
    "        \n",
    "        # Save last action\n",
    "        self.last_action = action\n",
    "        \n",
    "        return self.reward, self.planted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente Plantador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CollectingTractor(ap.Agent):\n",
    "    def setup(self):\n",
    "        \"\"\"Initialize agent parameters.\"\"\"\n",
    "        self.initialized = False\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset agent attributes.\"\"\"\n",
    "        if not self.initialized:\n",
    "            self._initialize_agent()\n",
    "        else:\n",
    "            self._restore_agent()\n",
    "\n",
    "    def _initialize_agent(self):\n",
    "        \"\"\"Initialize the agent's initial state.\"\"\"\n",
    "        self.planted = 0\n",
    "        self.destroyed = False\n",
    "        self.seeds = 0\n",
    "        self.type = 1\n",
    "        self.past_pos = (0, 0)\n",
    "        self.pos = (0, 0)\n",
    "        self.new_pos = (0, 0)\n",
    "        self.cell_state = 0 \n",
    "        self.last_action = 5 \n",
    "        self.last_planting_pos = None\n",
    "        self.steps_without_plant = 0\n",
    "        self.last_movAction = None\n",
    "        self.agentid = None\n",
    "        \n",
    "        # Initialize Q-learning parameters\n",
    "        self.q_table = np.zeros((self.p.grid_size, self.p.grid_size, self.p.capacity + 1, 2, 4, 7, 7))\n",
    "        self.learning_rate = self.p.learning_rate\n",
    "        self.discount_factor = self.p.discount_factor\n",
    "        self.epsilon = self.p.epsilon\n",
    "        \n",
    "        self.planted = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        # API\n",
    "        self.visitedPositions = []\n",
    "        \n",
    "    def setCellState(self, pos):\n",
    "        if self.model.np_grid[pos] == 5:\n",
    "            self.cell_state = 1\n",
    "        else:\n",
    "            self.cell_state = 0\n",
    "    \n",
    "    def exitGrid(self):\n",
    "        self.pos = (None, None)\n",
    "        \n",
    "    def setNewPositionVerify(self, pos):\n",
    "        self.new_pos = pos\n",
    "        \n",
    "    def setNewPositionArray(self, pos):\n",
    "        self.visitedPositions.append(pos)\n",
    "        \n",
    "    def move(self, agentes, dx=0, dy=0):\n",
    "        \"\"\"Move the tractor by (dx, dy).\"\"\"\n",
    "        x, y = self.pos\n",
    "        new_pos = (x + dx, y + dy)\n",
    "\n",
    "        # Asegúrate de que la nueva posición está dentro de los límites de la cuadrícula\n",
    "        if 0 <= new_pos[0] < self.p.grid_size and 0 <= new_pos[1] < self.p.grid_size:\n",
    "            # Verifica si la nueva posición no es un obstáculo\n",
    "            if self.model.np_grid[new_pos] != 2 and new_pos != self.p.seedsPositions[0]:\n",
    "                # Verifica si algún otro agente ya tiene la nueva posición como su siguiente posición\n",
    "                self.setNewPositionVerify(new_pos)\n",
    "                for agente in agentes:\n",
    "                    if agente is not self:\n",
    "                        if agente.new_pos == new_pos or agente.pos == new_pos:\n",
    "                            self.setNewPositionArray(self.pos)\n",
    "                            return self.pos, False  # No se puede mover porque la posición está ocupada por otro agente\n",
    "                # Si no hay obstáculos ni agentes en la nueva posición, se mueve\n",
    "                self.past_pos = self.pos\n",
    "                self.pos = new_pos\n",
    "                self.setCellState(self.pos)\n",
    "                self.setNewPositionArray(self.pos)\n",
    "                return self.pos, True\n",
    "        self.setNewPositionArray(self.pos)\n",
    "        return self.pos, False  # No se puede mover porque está fuera de los límites o hay un obstáculo\n",
    "\n",
    "    def move_up(self, agents):\n",
    "        self.last_movAction = 0\n",
    "        return self.move(agents, dy=1)\n",
    "\n",
    "    def move_down(self, agents):\n",
    "        self.last_movAction = 1\n",
    "        return self.move(agents, dy=-1)\n",
    "\n",
    "    def move_left(self, agents):\n",
    "        self.last_movAction = 2\n",
    "        return self.move(agents, dx=-1)\n",
    "\n",
    "    def move_right(self, agents):\n",
    "        self.last_movAction = 3\n",
    "        return self.move(agents, dx=1)\n",
    "    \n",
    "    def exit(self, agents):\n",
    "        x, y = self.pos\n",
    "\n",
    "        # Verifica si el agente ya está en el borde del grid\n",
    "        if (x == 0 or x == self.p.grid_size - 1 or y == 0 or y == self.p.grid_size - 1) and 3 not in self.model.np_grid:\n",
    "            self.destroyed = True\n",
    "            return self.pos, True  \n",
    "        else:\n",
    "            return self.pos, False\n",
    "\n",
    "    def drop(self, agents):\n",
    "        \"\"\"Drop seeds at the current target position.\"\"\"\n",
    "        if self.seeds > 0 and self.cell_state == 1:\n",
    "            self.seeds -= 1\n",
    "            # Remove target from grid\n",
    "            self.model.np_grid[self.pos] = 0\n",
    "            self.planted += 1\n",
    "            self.last_planting_pos = self.pos\n",
    "            self.steps_without_plant = 0\n",
    "            return self.pos, True\n",
    "        else:\n",
    "            return self.pos, False\n",
    "\n",
    "    def collect(self, agents):\n",
    "        \"\"\"Collect seeds if available at the surrounding positions, including diagonals.\"\"\"\n",
    "        # Verificar si el agente está en alguna de las celdas adyacentes\n",
    "        if self.pos in self.model.neighbors:\n",
    "            if self.seeds == 0:\n",
    "                self.seeds = self.capacity\n",
    "                return self.pos, 1\n",
    "            elif self.seeds > 0 and self.seeds < self.capacity:\n",
    "                self.seeds = self.capacity\n",
    "                return self.pos, 2\n",
    "\n",
    "        # Si el agente no está en ninguna celda adyacente a seedsPositions\n",
    "        return self.pos, 3 \n",
    "\n",
    "    def get_reward(self, action, accionado):\n",
    "        \"\"\"Calculate the reward based on the action and outcome.\"\"\"\n",
    "        reward = 0  # Base penalty for movement to encourage efficiency\n",
    "\n",
    "        if action in [0, 1, 2, 3]:  # Movement actions\n",
    "            if accionado:\n",
    "                reward -= 0 # Reduced penalty for valid movement\n",
    "            else:\n",
    "                reward -= 1000  # Strong penalty for hitting an obstacle or invalid move\n",
    "\n",
    "        elif action == 4:  # Drop seeds\n",
    "            if accionado:\n",
    "                reward += 1  # High reward for planting seeds\n",
    "            else:\n",
    "                reward -= 1000  # General penalty for unsuccessful action\n",
    "\n",
    "        elif action == 5:  # Collect seeds\n",
    "            if accionado == 1:  # If no seeds\n",
    "                reward += 0  # Reward for collecting seeds\n",
    "            elif accionado == 2:  # If has seeds but not full\n",
    "                reward += 0  # Reward for collecting seeds when not full\n",
    "            else:  # If not adjacent to seedsPositions\n",
    "                reward -= 1000  # Penalty for missing seeds\n",
    "\n",
    "        elif action == 6:  # Exit\n",
    "            if accionado:\n",
    "                reward += 1000\n",
    "            else:\n",
    "                reward -= 1000 \n",
    "                \n",
    "        # Penalize for not moving in a linear way\n",
    "        if action in [0, 1, 2, 3] and self.last_movAction is not None and self.last_movAction != action:\n",
    "            reward -= 5\n",
    "        \n",
    "        # Penalization for planting far from the last planting position\n",
    "        if self.last_planting_pos is not None:\n",
    "            last_x, last_y = self.last_planting_pos\n",
    "            current_x, current_y = self.pos\n",
    "            if abs(last_x - current_x) > 1 or abs(last_y - current_y) > 1:\n",
    "                reward -= 5\n",
    "                \n",
    "        \n",
    "        self.reward += reward\n",
    "        return reward\n",
    "\n",
    "        \n",
    "    def q_learning_update(self, state, action, reward, next_state):\n",
    "        pos_x, pos_y, seeds, closeAgent, leftResources, lastAction = state\n",
    "        next_pos_x, next_pos_y, next_seeds, next_closeAgent, next_leftResources, next_lastAction = next_state\n",
    "        \n",
    "        # Q-value actual considerando las celdas adyacentes\n",
    "        current_q = self.q_table[pos_x, pos_y, seeds, closeAgent, leftResources, lastAction, action]\n",
    "        # Max Q-value del siguiente estado\n",
    "        max_next_q = np.max(self.q_table[next_pos_x, next_pos_y, next_seeds, next_closeAgent, next_leftResources, next_lastAction])\n",
    "\n",
    "        # Actualizar el valor Q\n",
    "        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)\n",
    "        self.q_table[pos_x, pos_y, seeds, closeAgent, leftResources, lastAction, action] = new_q\n",
    "        \n",
    "        if self.destroyed:\n",
    "            self.exitGrid()\n",
    "\n",
    "    def acciones(self, action, agents):\n",
    "        \"\"\"Map action to the corresponding method.\"\"\"\n",
    "        action_map = {\n",
    "            0: self.move_up,\n",
    "            1: self.move_down,\n",
    "            2: self.move_left,\n",
    "            3: self.move_right,\n",
    "            4: self.drop,\n",
    "            5: self.collect,\n",
    "            6: self.exit\n",
    "        }\n",
    "        if action != 4:\n",
    "            self.steps_without_plant += 1\n",
    "        accion = action_map[action]\n",
    "        return accion(agents)\n",
    "\n",
    "    def closeAgents(self, agents):\n",
    "        for agente in agents:\n",
    "            if agente is not self:\n",
    "                if abs(agente.pos[0] - self.pos[0]) <= 2 and abs(agente.pos[1] - self.pos[1]) <= 2:\n",
    "                    return 1\n",
    "        return 0\n",
    "    \n",
    "    def leftResources(self):\n",
    "        remaining_resources = np.sum(self.model.np_grid == 3)\n",
    "        \n",
    "        if remaining_resources == 0:\n",
    "            return 0\n",
    "        elif remaining_resources < 0.3 * self.p.crops_count:\n",
    "            return 1\n",
    "        elif remaining_resources < 0.6 * self.p.crops_count:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    \n",
    "    def step(self, agents):\n",
    "        \"\"\"Execute a step in the agent's behavior.\"\"\"\n",
    "        if self.destroyed:\n",
    "            return \n",
    "            \n",
    "        # Definir el estado actual\n",
    "        closeAgents = self.closeAgents(agents)\n",
    "        leftResources = self.leftResources()\n",
    "        state = (self.pos[0], self.pos[1],  self.seeds, closeAgents, leftResources, self.last_action)\n",
    "        \n",
    "        # Acción seleccionada usando epsilon-greedy\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice([0, 1, 2, 3, 4, 5, 6])\n",
    "        else:\n",
    "            action = np.argmax(self.q_table[state[0], state[1], state[2], state[3], state[4], state[5]])\n",
    "            \n",
    "        # Ejecutar la acción y actualizar la Q-table\n",
    "        next_state, accionado = self.acciones(action, agents)\n",
    "        next_seeds = self.seeds  # Actualiza la cantidad de semillas después de la acción\n",
    "        next_closeAgents = self.closeAgents(agents)\n",
    "        next_leftResources = self.leftResources()\n",
    "        next_last_action = self.last_action\n",
    "        \n",
    "        # Asegúrate de que next_state esté dentro de los límites\n",
    "        next_state = (next_state[0], next_state[1], next_seeds, next_closeAgents, next_leftResources, next_last_action)\n",
    "        reward = self.get_reward(action, accionado)\n",
    "        self.q_learning_update(state, action, reward, next_state)\n",
    "        \n",
    "        # Save last action\n",
    "        self.last_action = action\n",
    "        \n",
    "        return self.reward, self.planted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TractorModel(ap.Model):\n",
    "    def setup(self):\n",
    "        \"\"\"Initialize the environment and generate coordinates.\"\"\"\n",
    "        self.initialized = False\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset or initialize the model state.\"\"\"\n",
    "        if not self.initialized:\n",
    "            self._initialize_environment()\n",
    "        else:\n",
    "            self._restore_environment()\n",
    "\n",
    "        self.steps = 0  # Reset the step counter\n",
    "\n",
    "    def _initialize_environment(self):\n",
    "        \"\"\"Initialize the environment, agents, and their targets.\"\"\"\n",
    "        self.steps = 0\n",
    "        self.max_steps = self.p.max_steps\n",
    "        self.np_grid = np.zeros((self.p.grid_size, self.p.grid_size))\n",
    "\n",
    "        # Generate and store obstacle positions\n",
    "        obastacle_pos, crop_pos = generate_grid(self, self.p.grid_size, self.p.obstacles_count, self.p.crops_count)\n",
    "        self.obstacle_positions = np.copy(self.grid[\"occupied\"])\n",
    "        self.crops_pos = crop_pos   \n",
    "        \n",
    "        # Create plant agents and assign initial properties\n",
    "        self.agents = ap.AgentList(self, self.p.number_of_tractors, CollectingTractor)\n",
    "        self.agents.extend(ap.AgentList(self, self.p.number_of_earthOnes, earthTractor))\n",
    "        self._initialize_agents(obastacle_pos, crop_pos)\n",
    "\n",
    "        self.grid.add_agents(self.agents, [tractor.pos for tractor in self.agents])\n",
    "        \n",
    "        # Assign agent id\n",
    "        for i, tractor in enumerate(self.agents):\n",
    "            tractor.agentid = i\n",
    "            \n",
    "                \n",
    "        for seed in self.p.seedsPositions:\n",
    "            self.np_grid[seed] = 4\n",
    "            \n",
    "        x, y = self.p.seedsPositions[0]\n",
    "        self.neighbors = [\n",
    "            (x-1, y), (x+1, y), (x, y-1), (x, y+1),  \n",
    "            (x-1, y-1), (x-1, y+1), (x+1, y-1), (x+1, y+1)  \n",
    "        ]\n",
    "        \n",
    "        for pos in self.neighbors:\n",
    "            self.np_grid[pos] = 0\n",
    "        \n",
    "        self.initialized = True  # Mark as initialized\n",
    "        \n",
    "    def _restore_environment(self):\n",
    "        \"\"\"Restore the environment to its initial state.\"\"\"\n",
    "        self.steps = 0\n",
    "        self.max_steps = self.p.max_steps\n",
    "        self.np_grid = np.zeros((self.p.grid_size, self.p.grid_size))\n",
    "        \n",
    "        # Restore obstacle positions\n",
    "        for x in range(self.p.grid_size):\n",
    "            for y in range(self.p.grid_size):\n",
    "                if self.obstacle_positions[x, y] == 1:\n",
    "                    self.np_grid[x, y] = 2 \n",
    "        \n",
    "        # Restore crop positions\n",
    "        for pos in self.crops_pos:\n",
    "            self.np_grid[pos] = 3\n",
    "            \n",
    "        self.agents.capacity = self.p.capacity\n",
    "        self.agents.seeds = self.p.starting_seeds\n",
    "        self.agents.destroyed = False\n",
    "        self.agents.steps_without_plant = 0\n",
    "        # Asignar las posiciones a los tractores\n",
    "        tractores = []\n",
    "        earthOnes = []\n",
    "        for tractor in self.agents:\n",
    "            if tractor.type == 1:\n",
    "                tractores.append(tractor)\n",
    "            elif tractor.type == 2:\n",
    "                earthOnes.append(tractor)\n",
    "        \n",
    "        for tractor, position in zip(tractores, self.p.tractor_positions):\n",
    "            tractor.pos = position\n",
    "        for tractor, position in zip(earthOnes, self.p.earthOnes_positions):\n",
    "            tractor.pos = position\n",
    "            \n",
    "        self.grid.add_agents(self.agents, [tractor.pos for tractor in self.agents])\n",
    "\n",
    "        # Restore agent id\n",
    "        for i, tractor in enumerate(self.agents):\n",
    "            tractor.agentid = i\n",
    "        \n",
    "        for seed in self.p.seedsPositions:\n",
    "            self.np_grid[seed] = 4\n",
    "            \n",
    "        x, y = self.p.seedsPositions[0]\n",
    "        self.neighbors = [\n",
    "            (x-1, y), (x+1, y), (x, y-1), (x, y+1),  \n",
    "            (x-1, y-1), (x-1, y+1), (x+1, y-1), (x+1, y+1)  \n",
    "        ]\n",
    "        \n",
    "        for pos in self.neighbors:\n",
    "            self.np_grid[pos] = 0\n",
    "\n",
    "    def _initialize_agents(self, obstacle_pos, crop_pos):\n",
    "        \"\"\"Initialize agents with positions and targets.\"\"\"\n",
    "        tractores = []\n",
    "        earthOnes = []\n",
    "        self.agents.destroyed = False\n",
    "        for tractor in self.agents:\n",
    "            tractor.capacity = self.p.capacity\n",
    "            tractor.seeds = self.p.starting_seeds\n",
    "            if tractor.type == 1:\n",
    "                tractores.append(tractor)\n",
    "            elif tractor.type == 2:  # corregir aquí\n",
    "                earthOnes.append(tractor)\n",
    "\n",
    "        # Asignar las posiciones a los tractores\n",
    "        for tractor, position in zip(tractores, self.p.tractor_positions):\n",
    "            tractor.pos = position\n",
    "\n",
    "        for tractor, position in zip(earthOnes, self.p.earthOnes_positions):\n",
    "            tractor.pos = position\n",
    "\n",
    "                    \n",
    "        self.coordsUsed = set()\n",
    "        self.coordsUsed.update(obstacle_pos)\n",
    "        self.coordsUsed.update(crop_pos)\n",
    "        \n",
    "    def _get_free_position(self):\n",
    "        \"\"\"Get a free position on the grid.\"\"\"\n",
    "        while True:\n",
    "            x, y = random.randint(0, self.p.grid_size - 1), random.randint(0, self.p.grid_size - 1)\n",
    "            if self.grid[\"occupied\"][(x, y)] == 0 and (x, y) not in self.coordsUsed and (x, y) != (0, 0):\n",
    "                return (x, y)\n",
    "\n",
    "    def step(self):\n",
    "         # Filtra los tractores activos\n",
    "        active_tractors = [tractor for tractor in self.agents if not tractor.destroyed]\n",
    "        for tractor in active_tractors:\n",
    "            reward, planted = tractor.step(self.agents)\n",
    "            if self.steps >= self.max_steps:\n",
    "                for tractor in self.agents:\n",
    "                    tractor.destroyed = True\n",
    "                self.end()\n",
    "        self.steps += 1\n",
    "        total_reward = sum([tractor.reward for tractor in self.agents])\n",
    "        total_planted = sum([tractor.planted for tractor in self.agents])\n",
    "        for tractor in self.agents:\n",
    "            tractor.reward = 0\n",
    "            tractor.planted = 0\n",
    "        return total_reward, total_planted\n",
    "\n",
    "    def end(self):\n",
    "        self.report('Total targets', self.agents.collected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar las imágenes una vez\n",
    "tractor_img = mpimg.imread('tractor.png')\n",
    "obstacle_img = mpimg.imread('obstacle.png')\n",
    "target_img = mpimg.imread('target.png')\n",
    "seeds_img = mpimg.imread('seeds.png')\n",
    "empty_img = mpimg.imread('empty.png')\n",
    "\n",
    "# Diccionario de imágenes\n",
    "images = {\n",
    "    'tractor': tractor_img,\n",
    "    'obstacle': obstacle_img,\n",
    "    'target': target_img,\n",
    "    'seeds': seeds_img,\n",
    "    'empty': empty_img\n",
    "}\n",
    "\n",
    "def save_frame(model, filename, total_reward, total_planted, images):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Asignar imágenes basadas en los valores de la cuadrícula\n",
    "    img_dict = {0: images['empty'], 2: images['obstacle'], 3: images['target'], 4: images['seeds']}\n",
    "    \n",
    "    # Mostrar la cuadrícula\n",
    "    for (x, y), value in np.ndenumerate(model.np_grid):\n",
    "        if value in img_dict:\n",
    "            ax.imshow(img_dict[value], extent=[y, y+1, x, x+1], aspect='auto')\n",
    "            \n",
    "    # Añadir tractores y texto\n",
    "    for agent in model.agents:\n",
    "        if agent.type == 1:\n",
    "            if agent.destroyed:\n",
    "                continue\n",
    "            ax.imshow(images['tractor'], extent=[agent.pos[1], agent.pos[1]+1, agent.pos[0], agent.pos[0]+1], aspect='auto')\n",
    "            ax.text(agent.pos[1] + 0.5, agent.pos[0] + 0.5, str(agent.seeds),\n",
    "                    color='black', fontsize=12, ha='center', va='center', weight='bold')\n",
    "        else:\n",
    "            if agent.destroyed:\n",
    "                continue\n",
    "            ax.imshow(images['tractor'], extent=[agent.pos[1], agent.pos[1]+1, agent.pos[0], agent.pos[0]+1], aspect='auto')\n",
    "\n",
    "    # Configurar límites y aspecto\n",
    "    ax.set_xlim([0, model.np_grid.shape[1]])\n",
    "    ax.set_ylim([0, model.np_grid.shape[0]])\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Título del gráfico\n",
    "    ax.set_title(f\"Tractor model \\n Reward: {total_reward} Planted: {total_planted}\")\n",
    "\n",
    "    # Guardar el cuadro\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def create_gif(filenames, gif_filename):\n",
    "    with imageio.get_writer(gif_filename, mode='I', duration=0.1) as writer:\n",
    "        for filename in filenames:\n",
    "            writer.append_data(imageio.imread(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_crops_count(grid_size, obstacles_count):\n",
    "    print(\"Total targets: \", grid_size**2 - obstacles_count)\n",
    "    return grid_size**2 - obstacles_count\n",
    "\n",
    "tractorParameters = { # NO MOVER NINGÚN PARÁMETRO\n",
    "    'grid_size': 12,\n",
    "    'obstacles_count': 7,\n",
    "    'number_of_earthOnes': 2,\n",
    "    'earthOnes_positions': [(0, 11), (11, 0)],\n",
    "    'number_of_tractors': 2,\n",
    "    'tractor_positions': [(0, 0), (11, 11)],\n",
    "    'max_steps': 300,\n",
    "    'seedsPositions': [(5, 5)],\n",
    "    'capacity': 5,\n",
    "    'starting_seeds': 5,\n",
    "    'learning_rate': 0.01, # No mover\n",
    "    'discount_factor': 1, # No mover\n",
    "    'epsilon': 1.0,\n",
    "    'crops_count': calculate_crops_count(12, 7)  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "rewards_per_episode = []\n",
    "epsilon_values = []\n",
    "planting_per_episode = []\n",
    "\n",
    "# Inicializar el modelo\n",
    "model = TractorModel(tractorParameters)\n",
    "model.setup()\n",
    "\n",
    "num_episodes = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(episode, save_gif=False):\n",
    "    model.reset()\n",
    "    total_reward = 0\n",
    "    total_planted = 0\n",
    "    \n",
    "    # Configura epsilon basado en el progreso de los episodios\n",
    "    epsilon_start, epsilon_end = 1.0, 0.0\n",
    "    if episode == num_episodes - 1:\n",
    "        epsilon = epsilon_end\n",
    "    elif episode < 0.00 * num_episodes:\n",
    "        epsilon = 1.0\n",
    "    else:\n",
    "        epsilon = max(epsilon_end, epsilon_start - (epsilon_start - epsilon_end) * episode / num_episodes)\n",
    "\n",
    "    # Asigna epsilon a cada tractor\n",
    "    for tractor in model.agents:\n",
    "        tractor.epsilon = epsilon\n",
    "\n",
    "    filenames = []\n",
    "    max_steps = model.max_steps  # Acceder una vez al valor de max_steps\n",
    "\n",
    "    with TemporaryDirectory() as temp_dir:\n",
    "        while model.steps < max_steps:\n",
    "            reward, planted = model.step()  # Ejecutar un paso en el modelo\n",
    "            total_reward += reward\n",
    "            total_planted += planted\n",
    "\n",
    "            if all(tractor.destroyed for tractor in model.agents):\n",
    "                break\n",
    "\n",
    "            if save_gif:\n",
    "                filename = os.path.join(temp_dir, f'frame_{model.steps}.png')\n",
    "                save_frame(model, filename, total_reward, total_planted, images)\n",
    "                filenames.append(filename)\n",
    "\n",
    "        if save_gif:\n",
    "            gif_filename = '1rstSimulacionTractores.gif' if episode == 0 else 'lastSimulacionTractores.gif'\n",
    "            create_gif(filenames, gif_filename)\n",
    "\n",
    "    return total_reward, epsilon, total_planted\n",
    "\n",
    "# Bucle principal de entrenamiento\n",
    "for episode in range(num_episodes):\n",
    "    save_gif = (episode == 0 or episode == num_episodes - 1)\n",
    "    total_reward, epsilon, total_planted = run_episode(episode, save_gif)\n",
    "\n",
    "    rewards_per_episode.append(total_reward)\n",
    "    epsilon_values.append(epsilon)\n",
    "    planting_per_episode.append(total_planted)\n",
    "\n",
    "    # Monitorear el progreso cada 10 episodios\n",
    "    if episode == num_episodes - 1 or episode % 100 == 0:\n",
    "        print(f'Episodio {episode + 1}/{num_episodes}, Recompensa total: {total_reward:.2f}, Epsilon: {epsilon:.2f}, Plantados: {total_planted:.2f}')\n",
    "        \n",
    "# Guardar cada tabla Q en un archivo con distintos nombres\n",
    "for i, tractor in enumerate(model.agents):\n",
    "    with open(f'q_table_{i}.pkl', 'wb') as f:\n",
    "        pickle.dump(tractor.q_table, f)\n",
    "\n",
    "print(\"Tablas Q guardadas exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de que rewards_per_episode y planting_per_episode sean arrays de NumPy\n",
    "rewards_per_episode = np.array(rewards_per_episode)\n",
    "planting_per_episode = np.array(planting_per_episode)\n",
    "\n",
    "# Colores para las gráficas\n",
    "color_reward = '#98FB98'  # Verde menta\n",
    "color_trend_line = '#FF6F61'  # Coral\n",
    "color_smoothed_planting = '#000'  # Negro\n",
    "color_epsilon = '#4682B4'  # Azul acero\n",
    "\n",
    "# Ajustar una línea recta (polinomio de grado 1) a los datos de recompensas\n",
    "z_rewards = np.polyfit(range(len(rewards_per_episode)), rewards_per_episode, 1)\n",
    "p_rewards = np.poly1d(z_rewards)\n",
    "\n",
    "# Crear la gráfica de recompensas\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(rewards_per_episode, label='Total Reward per Episode', color=color_reward)\n",
    "plt.plot(range(len(rewards_per_episode)), p_rewards(range(len(rewards_per_episode))), \"r--\", label='Trend Line', color=color_trend_line)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Reward per Episode')\n",
    "plt.legend()\n",
    "\n",
    "# Suavizar la serie de datos de plantación de semillas utilizando un promedio móvil\n",
    "window_size = 50\n",
    "smoothed_planting = np.convolve(planting_per_episode, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Ajustar una línea recta (polinomio de grado 1) a los datos de plantación suavizados\n",
    "z_planting = np.polyfit(range(len(smoothed_planting)), smoothed_planting, 1)\n",
    "p_planting = np.poly1d(z_planting)\n",
    "\n",
    "# Crear la gráfica de plantación de semillas\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(planting_per_episode, label='Planting per Episode', color=color_reward)\n",
    "plt.plot(range(len(smoothed_planting)), smoothed_planting, label='Smoothed Planting', color=color_smoothed_planting)\n",
    "plt.plot(range(len(smoothed_planting)), p_planting(range(len(smoothed_planting))), \"r--\", label='Trend Line', color=color_trend_line)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Planted')\n",
    "plt.title('Planting per Episode')\n",
    "plt.legend()\n",
    "\n",
    "# Crear la gráfica de epsilon\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epsilon_values, label='Epsilon', color=color_epsilon)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.title('Epsilon Decay')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí va una red capaz de aprender a jugar al juego de los tractores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulación Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el modelo\n",
    "model.reset()\n",
    "\n",
    "# Steps\n",
    "model.max_steps = 600\n",
    "\n",
    "# Establecer epsilon a 0 para la simulación final\n",
    "epsilon = 0\n",
    "for tractor in model.agents:\n",
    "    tractor.epsilon = epsilon\n",
    "\n",
    "# Cargar las tablas Q completadas para cada tractor\n",
    "for i, tractor in enumerate(model.agents):\n",
    "    with open(f'q_table_{i}.pkl', 'rb') as f:\n",
    "        tractor.q_table = pickle.load(f)\n",
    "\n",
    "# Variables para almacenar recompensas y plantados\n",
    "total_reward = 0\n",
    "total_planted = 0\n",
    "filenames = []\n",
    "\n",
    "# Directorio temporal para almacenar cuadros\n",
    "with TemporaryDirectory() as temp_dir:\n",
    "    while model.steps < model.max_steps:\n",
    "        reward, planted = model.step()  # Ejecutar un paso en el modelo\n",
    "        total_reward = reward\n",
    "        total_planted += planted\n",
    "\n",
    "        # Comprobar si todos los tractores están destruidos\n",
    "        if all(tractor.destroyed for tractor in model.agents):\n",
    "            break\n",
    "\n",
    "        # Guardar cuadro del estado actual del modelo\n",
    "        filename = os.path.join(temp_dir, f'frame_{model.steps}.png')\n",
    "        save_frame(model, filename, total_reward, total_planted, images)\n",
    "        filenames.append(filename)\n",
    "\n",
    "        # Imprimir progreso de la simulación\n",
    "        print(f\"Numero de pasos: {model.steps}\")\n",
    "        print(f\"Recompensa total: {total_reward:.2f}, Plantados: {total_planted:.2f}\")\n",
    "\n",
    "    # Crear GIF de la simulación final\n",
    "    gif_filename = 'simulacionFinal.gif'\n",
    "    create_gif(filenames, gif_filename)\n",
    "\n",
    "print(\"Simulación final guardada exitosamente en 'simulacionFinal.gif'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "def generate_initial_positions_json(model):\n",
    "    \"\"\"Generate JSON with initial positions of targets, obstacles, seeds, and tractors.\"\"\"\n",
    "    data = {\n",
    "        \"targets\": [],\n",
    "        \"obstacles\": [],\n",
    "        \"seedsPositions\": [],\n",
    "        \"tractors\": [],\n",
    "        \"grid\": tractorParameters['grid_size'],\n",
    "        \"max_steps\": tractorParameters['max_steps'],\n",
    "        \"capacity\": tractorParameters['capacity'],\n",
    "    }\n",
    "\n",
    "    # Recorrer el grid para identificar las posiciones\n",
    "    for x in range(model.p.grid_size):\n",
    "        for y in range(model.p.grid_size):\n",
    "            if model.np_grid[x, y] == 2:\n",
    "                data[\"obstacles\"].append([x, y])\n",
    "            elif model.np_grid[x, y] == 3:\n",
    "                data[\"targets\"].append([x, y])\n",
    "            elif model.np_grid[x, y] == 4:\n",
    "                data[\"seedsPositions\"].append([x, y])\n",
    "\n",
    "    # Obtener las posiciones iniciales de los tractores\n",
    "    for agent in model.agents:\n",
    "        if isinstance(agent, CollectingTractor):\n",
    "            data[\"tractors\"].append(agent.pos)\n",
    "\n",
    "    # Guardar en un archivo JSON\n",
    "    with open('initial_positions.json', 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    # Realizar la solicitud POST\n",
    "    response = requests.post('http://localhost:5000/initial_positions', json=data)\n",
    "    print(f'POST /initial_positions: {response.status_code} - {response.text}')\n",
    "\n",
    "def generate_tractor_paths_json(model):\n",
    "    \"\"\"Generate JSON with paths of each tractor.\"\"\"\n",
    "    data = {\n",
    "        \"tractor_paths\": {}\n",
    "    }\n",
    "\n",
    "    # Almacenar las posiciones de los tractores en cada paso\n",
    "    for agent in model.agents:\n",
    "        if isinstance(agent, CollectingTractor):\n",
    "            # Convertir el set a una lista antes de serializar\n",
    "            data[\"tractor_paths\"][agent.id] = list(agent.visitedPositions)\n",
    "\n",
    "    # Guardar en un archivo JSON\n",
    "    with open('tractor_paths.json', 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    # Realizar la solicitud POST\n",
    "    response = requests.post('http://localhost:5000/tractor_paths', json=data)\n",
    "    print(f'POST /tractor_paths: {response.status_code} - {response.text}')\n",
    "\n",
    "# Llamar a las funciones después de la simulación\n",
    "generate_initial_positions_json(model)\n",
    "generate_tractor_paths_json(model)\n",
    "\n",
    "\n",
    "# Emular un get de get tractor paths\n",
    "print(requests.get('http://localhost:5000/tractor_paths').json())\n",
    "print(requests.get('http://localhost:5000/initial_positions').json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
